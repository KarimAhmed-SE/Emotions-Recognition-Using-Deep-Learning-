# -*- coding: utf-8 -*-
"""Test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_smKYOHcmjWiLc4QmkOdP3K5zHaSBfqd
"""

import tensorflow as tf 
import cv2 
import os
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn
import skimage.io
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img
from tensorflow.python.keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D, Activation, GlobalAveragePooling2D
from tensorflow.keras.layers import BatchNormalization
import tensorflow as tf
from tensorflow import keras
import keras.backend as K
from tensorflow.keras.models import Sequential
from keras.models import Model

from google.colab import drive
drive.mount('/content/drive')

from zipfile import ZipFile
file_name = "/content/drive/MyDrive/archive.zip"

with ZipFile(file_name, 'r') as zip:
  zip.extractall()
  print('Done')

"""# **PREPROCESSING**


---


"""

Datadirectory = "train/"

Categories = ["angry", "disgusted", "fearful", "happy", "neutral", "sad", "surprised"]

for category in Categories:
  path = os.path.join(Datadirectory, category)
  for img in os.listdir(path):
    img_array = cv2.imread(os.path.join(path, img))
    plt.imshow(cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB))
    plt.show()
    break
  break

img_size = 128
newArray = cv2.resize(img_array, (img_size, img_size))
plt.imshow(cv2.cvtColor(newArray, cv2.COLOR_BGR2RGB))
plt.show

from types import new_class

training_Data = []
def create_training_data():
  for category in Categories:
    path = os.path.join(Datadirectory, category)
    class_num = Categories.index(category)
    for img in os.listdir(path):
      try:
        img_array = cv2.imread(os.path.join(path, img))
        newArray = cv2.resize(img_array, (img_size, img_size))
        training_Data.append([newArray, class_num])
      except Exception as e:
        pass

create_training_data()

X = []
y = []
for features, label in training_Data:

  X.append(features)
  y.append(label)


X = np.array(X).reshape(-1, img_size, img_size, 3)

# norm = cv2.normalize(X, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)

# from sklearn.preprocessing import LabelEncoder

# le = LabelEncoder()
# y = le.fit_transform(y)

X = X / 255.0

from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test = train_test_split(X,y,test_size=.2,random_state=0)

datagen_train = ImageDataGenerator(rescale=1./255,
                                   horizontal_flip = True,
                                  #  rotation_range = 10,
                                  #  zoom_range = 0.1,
                                  #  validation_split = 0.2,
)

train_generator = datagen_train.flow_from_directory('train/',
                                                batch_size = 128,
                                                target_size=(128, 128),
                                                shuffle=True,
                                                color_mode='rgb',
                                                # subset = 'training',
                                                class_mode='categorical')


datagen_valid = ImageDataGenerator(rescale=1./255, validation_split = 0.2)

valid_generator = datagen_train.flow_from_directory('train/',
                                                batch_size = 128,
                                                target_size=(128, 128),
                                                shuffle=True,
                                                color_mode='rgb',
                                                subset = 'validation',
                                                class_mode='categorical')

#sad 0000001
#happy 0000010
#angry 0000100

datagen_test = ImageDataGenerator(rescale=1./255)

test_generator = datagen_test.flow_from_directory('test/',
                                                batch_size = 128,
                                                target_size=(128, 128),
                                                shuffle=True,
                                                color_mode='rgb',
                                                class_mode='categorical')

"""# **MODEL BUILDING AND TRAINING**
# **VGG16 on FER2013**

---



"""

new_model = Sequential()


model = tf.keras.applications.VGG16(include_top = False, input_shape = (128,128,3), weights = 'imagenet', pooling = 'avg')
# model = Model (inputs=model.inputs, outputs= model.layers [-2].output)

# for layer in model.layers:
#   layer.trainable = False


# for layer in range(173):
#   model.layers[layer].trainable = False

for layer in model.layers[:-5]:
  layer.trainable = False

model.summary()

# batch_norm_indices = [2, 6, 9, 13, 14, 18, 21, 24, 28, 31, 34, 38, 41, 45, 46, 53, 56, 60, 63, 66, 70, 73, 76, 80, 83, 87, 88, 92, 95, 98, 102, 105, 108, 112, 115, 118, 122, 125, 128, 132, 135, 138, 142, 145, 149, 150, 154, 157, 160, 164, 167, 170]
# for i in range(170):
#     if i not in batch_norm_indices:
#         model.layers[i].trainable = False


new_model.add(model)

from keras.regularizers import l2
# new_model.add(Dropout(0.5))
new_model.add(Flatten())
new_model.add(BatchNormalization())
new_model.add(Dropout(0.25))
new_model.add(Dense(64, kernel_regularizer=l2(0.01),activation='relu'))
new_model.add(BatchNormalization())
new_model.add(Dropout(0.25))
new_model.add(Dense(64, kernel_regularizer=l2(0.01), activation='relu'))
new_model.add(BatchNormalization())
new_model.add(Dropout(0.25))
# new_model.add(Dense(64, kernel_regularizer=l2(0.01), activation='relu'))
# new_model.add(BatchNormalization())
# new_model.add(Dropout(0.25))
# new_model.add(Dense(512, kernel_regularizer=l2(0.01), activation='relu'))
# new_model.add(BatchNormalization())
# new_model.add(Dropout(0.25))
# new_model.add(Dense(256, activation='relu'))
# new_model.add(Dropout(0.4))
# new_model.add(Dense(64, activation='relu'))
# new_model.add(Dropout(0.2))
new_model.add(Dense(7, activation='softmax'))

new_model.summary()

from keras.optimizers import SGD
from keras.optimizers import Adam
adam = keras.optimizers.Adam(learning_rate=0.001)
opt = SGD(learning_rate=0.01)
sgd = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, decay=0.0001, nesterov=True)
new_model.compile(loss = "categorical_crossentropy", optimizer = adam, metrics = ["accuracy"])

from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler
reduce_lr = ReduceLROnPlateau(monitor='val_loss',
                              verbose=1,
                              patience=5,
                              min_lr=0.000001,
                              min_delta=0.1,
                              factor=0.2
                             )

earlystopping = EarlyStopping(monitor='val_accuracy', 
                              mode='max', 
                              verbose=1, 
                              patience=50
                             )

from keras.callbacks import ModelCheckpoint
mc = ModelCheckpoint(filepath="best_model.h5", monitor= 'val_accuracy', verbose= 1, save_best_only= True, mode = 'auto')

import math
batch_size = 64
trainingsize = 28709   
validate_size = 7178  
def calculate_spe(y):
  return int(math.ceil((1. * y) / batch_size)) 


steps_per_epoch = calculate_spe(trainingsize)
validation_steps = calculate_spe(validate_size)

history = new_model.fit(train_generator, validation_data = test_generator, shuffle = True, epochs = 100, callbacks=[reduce_lr, earlystopping])

score = new_model.evaluate(test_generator)

def Train_Val_Plot(acc,val_acc,loss,val_loss,auc,val_auc):
    
    fig, (ax1, ax2,ax3,ax4,ax5) = plt.subplots(1,5, figsize= (20,5))
    fig.suptitle(" MODEL'S METRICS VISUALIZATION ")

    ax1.plot(range(1, len(acc) + 1), acc)
    ax1.plot(range(1, len(val_acc) + 1), val_acc)
    ax1.set_title('History of Accuracy')
    ax1.set_xlabel('Epochs')
    ax1.set_ylabel('Accuracy')
    ax1.legend(['training', 'validation'])


    ax2.plot(range(1, len(loss) + 1), loss)
    ax2.plot(range(1, len(val_loss) + 1), val_loss)
    ax2.set_title('History of Loss')
    ax2.set_xlabel('Epochs')
    ax2.set_ylabel('Loss')
    ax2.legend(['training', 'validation'])
    
    ax3.plot(range(1, len(auc) + 1), auc)
    ax3.plot(range(1, len(val_auc) + 1), val_auc)
    ax3.set_title('History of AUC')
    ax3.set_xlabel('Epochs')
    ax3.set_ylabel('AUC')
    ax3.legend(['training', 'validation'])
    

    plt.show()

Train_Val_Plot(history.history['accuracy'],history.history['val_accuracy'],
               history.history['loss'],history.history['val_loss'],
               history.history['accuracy'],history.history['val_accuracy']
              )

from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
def performance_metrics(model, test):
    y_pred = new_model.predict(test)
    y_result = []
    for p in y_pred:
        y_result.append(np.argmax(p))
    
    y_actual = []
    for i in range(len(test)):
        for p in test[i][1]:
            y_actual.append(np.argmax(p))
    
    print(classification_report(y_actual, y_result))
    
    
    cm = tf.math.confusion_matrix(labels = y_actual, predictions = y_result)

    plt.figure(figsize = (10, 7))
    sn.heatmap(cm, annot = True, fmt = 'd')
    plt.xlabel('Predicted')
    plt.ylabel('Truth')


performance_metrics(new_model, test_generator)

new_model.save('TrainedResNet50.h5')

Loaded_model = keras.models.load_model('TrainedResNet50.h5')

feature_extraction = new_model.predict(X_train)

features = feature_extraction.reshape(feature_extraction.shape[0], -1)

"""# **SVM CLASSIFICATION**

---


"""

X_for_SVM = features

from sklearn import svm
clf = svm.SVC()
clf.fit(X_for_SVM, Y_train)

X_Test_feature = new_model.predict(X_test)

X_Test_New_Features = X_Test_feature.reshape(X_Test_feature.shape[0], -1)

y_pred = clf.predict(X_Test_New_Features)

from sklearn import metrics
print("Accuracy:",metrics.accuracy_score(Y_test, y_pred)*100)

"""# **CK+ Dataset**

---


"""

import tensorflow as tf 
import cv2 
import os
import matplotlib.pyplot as plt
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img
from tensorflow.python.keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D, Activation, GlobalAveragePooling2D
from tensorflow.keras.layers import BatchNormalization
import tensorflow as tf
from tensorflow import keras
import keras.backend as K
from tensorflow.keras.models import Sequential
from keras.models import Model

from zipfile import ZipFile
file_name = "/content/drive/MyDrive/CK+.zip"

with ZipFile(file_name, 'r') as zip:
  zip.extractall()
  print('Done')

DatadirectoryCK = "CK+48/"

CategoriesCK = ["anger", "contempt", "disgust", "fear", "happy", "sadness", "surprise"]

for category in CategoriesCK:
  path = os.path.join(DatadirectoryCK, category)
  for img in os.listdir(path):
    img_array = cv2.imread(os.path.join(path, img))
    plt.imshow(cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB))
    plt.show()
    break
  break

img_size = 128
newArray = cv2.resize(img_array, (img_size, img_size))
plt.imshow(cv2.cvtColor(newArray, cv2.COLOR_BGR2RGB))
plt.show

from types import new_class

training_DataCK = []
def create_training_data():
  for category in CategoriesCK:
    path = os.path.join(DatadirectoryCK, category)
    class_num = CategoriesCK.index(category)
    for img in os.listdir(path):
      try:
        img_array = cv2.imread(os.path.join(path, img))
        newArray = cv2.resize(img_array, (img_size, img_size))
        training_DataCK.append([newArray, class_num])
      except Exception as e:
        pass

create_training_data()

XCK = []
yCK = []
for features, label in training_DataCK:

  XCK.append(features)
  yCK.append(label)


XCK = np.array(XCK).reshape(-1, img_size, img_size, 3)

XCK = XCK / 255.0

from sklearn.model_selection import train_test_split
X_trainCK,X_testCK,Y_trainCK,Y_testCK = train_test_split(XCK,yCK,test_size=.2,random_state=0)

new_modelCK = Sequential()


modelCK = tf.keras.applications.VGG16(include_top = False, input_shape = (128,128,3), weights = 'imagenet', pooling = 'avg')

# for layer in modelCK.layers:
#   layer.trainable = False


for layer in modelCK.layers[:-5]:
  layer.trainable = False


# model.summary()

# batch_norm_indices = [2, 6, 9, 13, 14, 18, 21, 24, 28, 31, 34, 38, 41, 45, 46, 53, 56, 60, 63, 66, 70, 73, 76, 80, 83, 87, 88, 92, 95, 98, 102, 105, 108, 112, 115, 118, 122, 125, 128, 132, 135, 138, 142, 145, 149, 150, 154, 157, 160, 164, 167, 170]
# for i in range(170):
#     if i not in batch_norm_indices:
#         modelCK.layers[i].trainable = False


new_modelCK.add(modelCK)

from keras.regularizers import l2
from tensorflow.keras import regularizers

new_modelCK.add(Flatten())
# new_modelCK.add(BatchNormalization())
# new_modelCK.add(Dropout(0.25))
new_modelCK.add(Dense(64,kernel_regularizer=l2(0.01), activation='relu'))
new_modelCK.add(BatchNormalization())
new_modelCK.add(Dropout(0.4))
new_modelCK.add(Dense(64,kernel_regularizer=l2(0.01), activation='relu'))
new_modelCK.add(BatchNormalization())
new_modelCK.add(Dropout(0.4))


# new_modelCK.add(Dense(64,kernel_regularizer=l2(0.01), activation='relu'))
# new_modelCK.add(BatchNormalization())
# new_modelCK.add(Dropout(0.25))
# new_modelCK.add(Dense(64,kernel_regularizer=l2(0.01), activation='relu'))
# new_modelCK.add(BatchNormalization())
# new_modelCK.add(Dropout(0.25))
new_modelCK.add(Dense(7, activation='softmax'))



from keras.optimizers import SGD
from keras.optimizers import Adam
adam = keras.optimizers.Adam(learning_rate=0.0001)
opt = SGD(learning_rate=0.001)
new_modelCK.compile(loss = "categorical_crossentropy", optimizer = adam, metrics = ["accuracy"])

from tensorflow.keras.utils import plot_model
from IPython.display import Image
plot_model(new_modelCK, to_file='convnet.png', show_shapes=True,show_layer_names=True)
Image(filename='convnet.png')

from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler
reduce_lr = ReduceLROnPlateau(monitor='val_loss',
                              verbose=1,
                              patience=10,
                              min_lr=0.000001,
                              min_delta=0.1,
                              factor=0.2
                             )

earlystopping = EarlyStopping(monitor='val_accuracy', 
                              mode='max', 
                              verbose=1, 
                              patience=20
                             )

train_datagen = ImageDataGenerator(rescale=1./255,
                                   horizontal_flip = True,
                                   rotation_range = 20,
                                   zoom_range = 0.2,
                                   shear_range = 0.2,
                                   width_shift_range = 0.2,
                                   height_shift_range = 0.2,
                                   validation_split = 0.3)

valid_datagen = ImageDataGenerator(rescale = 1./255,
                                  validation_split = 0.3)

train_dataset  = train_datagen.flow_from_directory(directory = '/content/CK+48',
                                                   target_size = (128,128),
                                                   class_mode = 'categorical',
                                                   color_mode = 'rgb',
                                                   shuffle = True,
                                                   subset = 'training',
                                                   batch_size = 32)

valid_dataset = valid_datagen.flow_from_directory(directory = '/content/CK+48',
                                                  target_size = (128,128),
                                                  color_mode = 'rgb',
                                                  shuffle = True,
                                                  class_mode = 'categorical',
                                                  subset = 'validation',
                                                  batch_size = 32)

historyCK = new_modelCK.fit(train_dataset, validation_data = valid_dataset, epochs = 100, callbacks=[reduce_lr, earlystopping], shuffle = True)

scoreCK = new_modelCK.evaluate(valid_dataset)

def Train_Val_Plot(acc,val_acc,loss,val_loss,auc,val_auc):
    
    fig, (ax1, ax2,ax3,ax4,ax5) = plt.subplots(1,5, figsize= (20,5))
    fig.suptitle(" MODEL'S METRICS VISUALIZATION ")

    ax1.plot(range(1, len(acc) + 1), acc)
    ax1.plot(range(1, len(val_acc) + 1), val_acc)
    ax1.set_title('History of Accuracy')
    ax1.set_xlabel('Epochs')
    ax1.set_ylabel('Accuracy')
    ax1.legend(['training', 'validation'])


    ax2.plot(range(1, len(loss) + 1), loss)
    ax2.plot(range(1, len(val_loss) + 1), val_loss)
    ax2.set_title('History of Loss')
    ax2.set_xlabel('Epochs')
    ax2.set_ylabel('Loss')
    ax2.legend(['training', 'validation'])
    
    ax3.plot(range(1, len(auc) + 1), auc)
    ax3.plot(range(1, len(val_auc) + 1), val_auc)
    ax3.set_title('History of AUC')
    ax3.set_xlabel('Epochs')
    ax3.set_ylabel('AUC')
    ax3.legend(['training', 'validation'])
    

    plt.show()
    

Train_Val_Plot(historyCK.history['accuracy'],historyCK.history['val_accuracy'],
               historyCK.history['loss'],historyCK.history['val_loss'],
               historyCK.history['accuracy'],historyCK.history['val_accuracy']
              )

feature_extractionCK = new_modelCK.predict(X_trainCK)

featuresCK = feature_extractionCK.reshape(feature_extractionCK.shape[0], -1)

X_for_SVMCK = featuresCK

from sklearn import svm
clfCK = svm.SVC()
clfCK.fit(X_for_SVMCK, Y_trainCK)

X_Test_featureCK = new_modelCK.predict(X_testCK)

X_Test_New_FeaturesCK = X_Test_featureCK.reshape(X_Test_featureCK.shape[0], -1)

y_predCK = clfCK.predict(X_Test_New_FeaturesCK)

from sklearn import metrics
print("Accuracy:",metrics.accuracy_score(Y_testCK, y_predCK)*100)

cm = tf.math.confusion_matrix(Y_testCK, y_predCK)
ConfusionMatrixDisplay.from_predictions(Y_testCK, y_predCK)
plt.show()



from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
def performance_metrics(model, test):
    y_pred = new_modelCK.predict(test)
    y_result = []
    for p in y_pred:
        y_result.append(np.argmax(p))
    
    y_actual = []
    for i in range(len(test)):
        for p in test[i][1]:
            y_actual.append(np.argmax(p))
    
    print(classification_report(y_actual, y_result))
    
    

    plt.figure(figsize = (10, 7))
    sn.heatmap(cm, annot = True, fmt = 'd')
    plt.xlabel('Predicted')
    plt.ylabel('Truth')


performance_metrics(new_modelCK, valid_dataset)

"""# **RAF Dataset**"""

from zipfile import ZipFile
file_name = "/content/drive/MyDrive/RAF-DB.zip"

with ZipFile(file_name, 'r') as zip:
  zip.extractall()
  print('Done')

XrafTrain= [] #empty list of dataset
dataset_path = '/content/dataset/aligned/train/train_images'
for file_name in os.listdir (dataset_path):
  img = cv2.imread(os.path.join (dataset_path, file_name))
  img = cv2.resize (img, (128,128))
  XrafTrain.append (img)


XrafTrain = np.array (XrafTrain)
print (XrafTrain.shape)

labelsTraining = pd.read_csv('/content/dataset/train_labels.csv')

yTrain = labelsTraining.iloc[:, -1]

XrafTest= [] #empty list of dataset
dataset_path = '/content/dataset/aligned/test/test_images'
for file_name in os.listdir (dataset_path):
  img = cv2.imread(os.path.join (dataset_path, file_name))
  img = cv2.resize (img, (128,128))
  XrafTest.append (img)

  
XrafTest = np.array (XrafTest)
print (XrafTest.shape)

labelsTesting = pd.read_csv('/content/dataset/test_labels.csv')

yTest = labelsTesting.iloc[:, -1]

datagen_train = ImageDataGenerator(rescale=1./255,
                                   horizontal_flip = True,
                                   rotation_range = 20,
                                   zoom_range = 0.2,
                                   shear_range = 0.2,
                                   width_shift_range = 0.2,
                                   height_shift_range = 0.2
)

# dls = ImageDataLoaders.from_csv(
#     path='/content/dataset/aligned',
#     csv_fname='/content/dataset/train_labels.csv',
#     fn_col=0,
#     suff='.tif',
#     folder='train',
#     label_col=1,
# )

train_generator = datagen_train.flow_from_directory('/content/dataset/aligned/train/',
                                                batch_size = 128,
                                                target_size=(128, 128),
                                                shuffle=True,
                                                color_mode='rgb',
                                                class_mode='categorical')


datagen_test = ImageDataGenerator(rescale=1./255, validation_split = 0.2)

test_generator = datagen_train.flow_from_directory('/content/dataset/aligned/test/',
                                                batch_size = 128,
                                                target_size=(128, 128),
                                                shuffle=True,
                                                color_mode='rgb',
                                                class_mode='categorical')

new_modelRaf = Sequential()


modelraf = tf.keras.applications.VGG16(include_top = False, input_shape = (128,128,3), weights = 'imagenet', pooling = 'avg')

# for layer in modelCK.layers:
#   layer.trainable = False


for layer in modelraf.layers[:-5]:
  layer.trainable = False


# model.summary()

# batch_norm_indices = [2, 6, 9, 13, 14, 18, 21, 24, 28, 31, 34, 38, 41, 45, 46, 53, 56, 60, 63, 66, 70, 73, 76, 80, 83, 87, 88, 92, 95, 98, 102, 105, 108, 112, 115, 118, 122, 125, 128, 132, 135, 138, 142, 145, 149, 150, 154, 157, 160, 164, 167, 170]
# for i in range(170):
#     if i not in batch_norm_indices:
#         modelCK.layers[i].trainable = False


new_modelRaf.add(modelraf)

from keras.regularizers import l2
from tensorflow.keras import regularizers

new_modelRaf.add(Flatten())
# new_modelCK.add(BatchNormalization())
# new_modelCK.add(Dropout(0.25))
new_modelRaf.add(Dense(64,kernel_regularizer=l2(0.01), activation='relu'))
new_modelRaf.add(BatchNormalization())
new_modelRaf.add(Dropout(0.4))
new_modelRaf.add(Dense(64,kernel_regularizer=l2(0.01), activation='relu'))
new_modelRaf.add(BatchNormalization())
new_modelRaf.add(Dropout(0.4))


# new_modelCK.add(Dense(64,kernel_regularizer=l2(0.01), activation='relu'))
# new_modelCK.add(BatchNormalization())
# new_modelCK.add(Dropout(0.25))
# new_modelCK.add(Dense(64,kernel_regularizer=l2(0.01), activation='relu'))
# new_modelCK.add(BatchNormalization())
# new_modelCK.add(Dropout(0.25))
new_modelRaf.add(Dense(7, activation='softmax'))

from keras.optimizers import SGD
from keras.optimizers import Adam
adam = keras.optimizers.Adam(learning_rate=0.0001)
opt = SGD(learning_rate=0.001)
new_modelRaf.compile(loss = "categorical_crossentropy", optimizer = adam, metrics = ["accuracy"])

from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler
reduce_lr = ReduceLROnPlateau(monitor='val_loss',
                              verbose=1,
                              patience=10,
                              min_lr=0.000001,
                              min_delta=0.1,
                              factor=0.2
                             )

earlystopping = EarlyStopping(monitor='val_accuracy', 
                              mode='max', 
                              verbose=1, 
                              patience=20
                             )

historyRaf = new_modelRaf.fit(XrafTrain, yTrain, epochs = 100, callbacks=[reduce_lr, earlystopping], shuffle = True)

"""# **KDEF Dataset**"""

from zipfile import ZipFile
file_name = "/content/drive/MyDrive/KDEFDataset.zip"

with ZipFile(file_name, 'r') as zip:
  zip.extractall()
  print('Done')

DatadirectoryKDEF = "KDEF/"

CategoriesKDEF = ["angry", "disgust", "fear", "happy", "neutral", "sad", "surprise"]

for category in CategoriesKDEF:
  path = os.path.join(DatadirectoryKDEF, category)
  for img in os.listdir(path):
    img_array = cv2.imread(os.path.join(path, img))
    plt.imshow(cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB))
    plt.show()
    break
  break

img_size = 128
newArray = cv2.resize(img_array, (img_size, img_size))
plt.imshow(cv2.cvtColor(newArray, cv2.COLOR_BGR2RGB))
plt.show

from types import new_class

training_DataKDEF = []
def create_training_data():
  for category in CategoriesKDEF:
    path = os.path.join(DatadirectoryKDEF, category)
    class_num = CategoriesKDEF.index(category)
    for img in os.listdir(path):
      try:
        img_array = cv2.imread(os.path.join(path, img))
        newArray = cv2.resize(img_array, (img_size, img_size))
        training_DataKDEF.append([newArray, class_num])
      except Exception as e:
        pass

create_training_data()

XKDEF = []
yKDEF = []
for features, label in training_DataKDEF:

  XKDEF.append(features)
  yKDEF.append(label)


XKDEF = np.array(XKDEF).reshape(-1, img_size, img_size, 3)

XKDEF = XKDEF / 255.0

from sklearn.model_selection import train_test_split
X_trainKDEF,X_testKDEF,Y_trainKDEF,Y_testKDEF = train_test_split(XKDEF,yKDEF,test_size=.2,random_state=0)

datagen_trainKDEF = ImageDataGenerator(rescale=1./255,
                                   horizontal_flip = True,
                                   rotation_range = 10,
                                   zoom_range = 0.1,
                                   shear_range = 0.1,
                                   width_shift_range = 0.1,
                                   height_shift_range = 0.1,
                                   validation_split = 0.2
)


train_generatorKDEF = datagen_trainKDEF.flow_from_directory('/content/KDEF',
                                                batch_size = 128,
                                                target_size=(128, 128),
                                                shuffle=True,
                                                color_mode='rgb',
                                                subset = 'training',
                                                class_mode='categorical')


datagen_validKDEF = ImageDataGenerator(rescale=1./255, validation_split = 0.2)

valid_generatorKDEF = datagen_trainKDEF.flow_from_directory('/content/KDEF',
                                                batch_size = 128,
                                                target_size=(128, 128),
                                                shuffle=True,
                                                subset = 'validation',
                                                color_mode='rgb',
                                                class_mode='categorical')

# datagen_test = ImageDataGenerator(rescale=1./255)

# test_generator = datagen_train.flow_from_directory('/content/KDEF',
#                                                 batch_size = 128,
#                                                 target_size=(128, 128),
#                                                 shuffle=True,
#                                                 color_mode='rgb',
#                                                 class_mode='categorical')

new_modelKDEF = Sequential()


modelKDEF = tf.keras.applications.VGG16(include_top = False, input_shape = (128,128,3), weights = 'imagenet', pooling = 'avg')

# for layer in modelCK.layers:
#   layer.trainable = False


for layer in modelKDEF.layers[:-5]:
  layer.trainable = False


# model.summary()

# batch_norm_indices = [2, 6, 9, 13, 14, 18, 21, 24, 28, 31, 34, 38, 41, 45, 46, 53, 56, 60, 63, 66, 70, 73, 76, 80, 83, 87, 88, 92, 95, 98, 102, 105, 108, 112, 115, 118, 122, 125, 128, 132, 135, 138, 142, 145, 149, 150, 154, 157, 160, 164, 167, 170]
# for i in range(170):
#     if i not in batch_norm_indices:
#         modelCK.layers[i].trainable = False


new_modelKDEF.add(modelKDEF)

from keras.regularizers import l2
from tensorflow.keras import regularizers

new_modelKDEF.add(Flatten())
# new_modelCK.add(BatchNormalization())
# new_modelCK.add(Dropout(0.25))
new_modelKDEF.add(Dense(64,kernel_regularizer=l2(0.01), activation='relu'))
new_modelKDEF.add(BatchNormalization())
new_modelKDEF.add(Dropout(0.4))
new_modelKDEF.add(Dense(64,kernel_regularizer=l2(0.01), activation='relu'))
new_modelKDEF.add(BatchNormalization())
new_modelKDEF.add(Dropout(0.4))


# new_modelCK.add(Dense(64,kernel_regularizer=l2(0.01), activation='relu'))
# new_modelCK.add(BatchNormalization())
# new_modelCK.add(Dropout(0.25))
# new_modelCK.add(Dense(64,kernel_regularizer=l2(0.01), activation='relu'))
# new_modelCK.add(BatchNormalization())
# new_modelCK.add(Dropout(0.25))
new_modelKDEF.add(Dense(7, activation='softmax'))

from keras.optimizers import SGD
from keras.optimizers import Adam
adam = keras.optimizers.Adam(learning_rate=0.0001)
opt = SGD(learning_rate=0.001)
new_modelKDEF.compile(loss = "categorical_crossentropy", optimizer = adam, metrics = ["accuracy"])

from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler
reduce_lr = ReduceLROnPlateau(monitor='val_loss',
                              verbose=1,
                              patience=10,
                              min_lr=0.000001,
                              min_delta=0.1,
                              factor=0.2
                             )

earlystopping = EarlyStopping(monitor='val_accuracy', 
                              mode='max', 
                              verbose=1, 
                              patience=20
                             )

historyKDEF = new_modelKDEF.fit(train_generatorKDEF, validation_data = valid_generatorKDEF, epochs = 60, callbacks=[reduce_lr, earlystopping], shuffle = True)

scoreKDEF = new_modelKDEF.evaluate(valid_generatorKDEF)

def Train_Val_Plot(acc,val_acc,loss,val_loss,auc,val_auc):
    
    fig, (ax1, ax2,ax3,ax4,ax5) = plt.subplots(1,5, figsize= (20,5))
    fig.suptitle(" MODEL'S METRICS VISUALIZATION ON KDEF DATASET")

    ax1.plot(range(1, len(acc) + 1), acc)
    ax1.plot(range(1, len(val_acc) + 1), val_acc)
    ax1.set_title('History of Accuracy on KDEF')
    ax1.set_xlabel('Epochs')
    ax1.set_ylabel('Accuracy')
    ax1.legend(['training', 'validation'])


    ax2.plot(range(1, len(loss) + 1), loss)
    ax2.plot(range(1, len(val_loss) + 1), val_loss)
    ax2.set_title('History of Loss on KDEF')
    ax2.set_xlabel('Epochs')
    ax2.set_ylabel('Loss')
    ax2.legend(['training', 'validation'])
    
    ax3.plot(range(1, len(auc) + 1), auc)
    ax3.plot(range(1, len(val_auc) + 1), val_auc)
    ax3.set_title('History of AUC on KDEF')
    ax3.set_xlabel('Epochs')
    ax3.set_ylabel('AUC')
    ax3.legend(['training', 'validation'])
    

    plt.show()
    

Train_Val_Plot(historyKDEF.history['accuracy'],historyKDEF.history['val_accuracy'],
               historyKDEF.history['loss'],historyKDEF.history['val_loss'],
               historyKDEF.history['accuracy'],historyKDEF.history['val_accuracy']
              )

from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
def performance_metrics(model, test):
    y_pred = new_modelKDEF.predict(test)
    y_result = []
    for p in y_pred:
        y_result.append(np.argmax(p))
    
    y_actual = []
    for i in range(len(test)):
        for p in test[i][1]:
            y_actual.append(np.argmax(p))
    
    print(classification_report(y_actual, y_result))
    
    
    cm = tf.math.confusion_matrix(labels = y_actual, predictions = y_result)

    plt.figure(figsize = (10, 7))
    sn.heatmap(cm, annot = True, fmt = 'd')
    plt.xlabel('Predicted')
    plt.ylabel('Truth')


performance_metrics(new_modelKDEF, valid_generatorKDEF)

feature_extractionKDEF = new_modelKDEF.predict(X_trainKDEF)

featuresKDEF = feature_extractionKDEF.reshape(feature_extractionKDEF.shape[0], -1)

X_for_SVMKDEF = featuresKDEF

from sklearn import svm
clfKDEF = svm.SVC()
clfKDEF.fit(X_for_SVMKDEF, Y_trainKDEF)

X_Test_featureKDEF = new_modelKDEF.predict(X_testKDEF)

X_Test_New_FeaturesKDEF = X_Test_featureKDEF.reshape(X_Test_featureKDEF.shape[0], -1)

y_predKDEF = clfKDEF.predict(X_Test_New_FeaturesKDEF)

from sklearn import metrics
print("Accuracy:",metrics.accuracy_score(Y_testKDEF, y_predKDEF)*100)

"""# **TESTING**"""

from keras.preprocessing import image
from keras.applications.vgg16 import VGG16
from keras.applications.vgg16 import preprocess_input
import numpy as np
import  numpy  as  np
from keras.layers import *
from keras.applications.vgg16 import VGG16
from keras.applications.vgg16 import preprocess_input
from keras.layers import Dropout, Flatten, Dense
from keras.applications import ResNet50
from keras.models import Model, Sequential
from keras.layers import Dense, GlobalAveragePooling2D
from keras import backend as K
import matplotlib.pyplot as plt

img_path = '/content/test/happy/im100.png'
img = tf.keras.utils.load_img(img_path, target_size=(128, 128))
img_data = tf.keras.utils.img_to_array(img)
img_data = np.expand_dims(img_data, axis=0)
img_data = preprocess_input(img_data)

print(img_data.shape)

testing = new_modelCK.predict(img_data)

Testing_fixed = testing.reshape(testing.shape[0], -1)

testing[0]

y_predCKTest = clfCK.predict(Testing_fixed)

print(y_predCKTest)

np.argmax(testing)